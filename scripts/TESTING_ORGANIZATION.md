# Test Script Organization

## Directory Structure

```
scripts/
â”œâ”€â”€ integration/              # Integration test scripts (core testing)
â”‚   â”œâ”€â”€ run_integration_test.sh
â”‚   â”œâ”€â”€ run_integration_test_detailed.sh  
â”‚   â”œâ”€â”€ test_docker_install.sh
â”‚   â””â”€â”€ test_mcp_tools.py
â”œâ”€â”€ unit/                     # Unit test helpers
â”‚   â”œâ”€â”€ pre_commit_validation.sh
â”‚   â””â”€â”€ docker_test_runner.sh
â”œâ”€â”€ analysis/                 # Content analysis and investigation
â”‚   â”œâ”€â”€ analyze_customer_messages.py
â”‚   â”œâ”€â”€ analyze_conversation_updates_multidate.py
â”‚   â”œâ”€â”€ content_verification.py  (NEW)
â”‚   â””â”€â”€ investigate_conversation_updates.py
â”œâ”€â”€ performance/              # Performance testing
â”‚   â”œâ”€â”€ performance_test.py
â”‚   â”œâ”€â”€ quick_performance_test.py
â”‚   â””â”€â”€ generate_performance_report.py
â”œâ”€â”€ utilities/                # One-time investigation tools
â”‚   â”œâ”€â”€ debug_sync_issue.py
â”‚   â”œâ”€â”€ generate_test_data.py
â”‚   â”œâ”€â”€ import_test_data.py
â”‚   â”œâ”€â”€ monitor_sync_progress.py
â”‚   â”œâ”€â”€ quick_test.py
â”‚   â”œâ”€â”€ simple_analysis.py
â”‚   â”œâ”€â”€ simple_analysis_multidate.py
â”‚   â””â”€â”€ test_sync_specific_date.py
â””â”€â”€ baseline/                 # Baseline and comparison tools
    â”œâ”€â”€ record_baseline.py
    â””â”€â”€ compare_to_baseline.py
```

## Script Categories

### ðŸ§ª **Integration Tests** (Core Testing Pipeline)
**Purpose**: Verify the system works end-to-end  
**When to run**: Before PRs, after major changes, scheduled CI  
**Expected runtime**: 2-15 minutes

- `run_integration_test.sh` - Main integration test (1-7 days data)
- `run_integration_test_detailed.sh` - With persistent logging  
- `test_docker_install.sh` - Docker deployment testing
- `test_mcp_tools.py` - MCP protocol validation

### ðŸ”¬ **Unit Tests** (Code Quality)
**Purpose**: Validate code quality and unit functionality  
**When to run**: Pre-commit, fast feedback  
**Expected runtime**: 30 seconds - 2 minutes

- `pre_commit_validation.sh` - Linting, type checking, unit tests
- `docker_test_runner.sh` - CI environment parity

### ðŸ“Š **Analysis Scripts** (Content Verification)
**Purpose**: Verify data quality and service behavior  
**When to run**: After integration tests, investigate issues  
**Expected runtime**: 1-5 minutes

- `analyze_customer_messages.py` - Customer vs admin message analysis
- `content_verification.py` - NEW: Comprehensive content analysis
- `analyze_conversation_updates_multidate.py` - Multi-date investigation

### âš¡ **Performance Tests** (Performance Validation)
**Purpose**: Ensure performance targets are met  
**When to run**: After optimization changes, periodic monitoring  
**Expected runtime**: 2-10 minutes

- `performance_test.py` - Comprehensive performance testing
- `quick_performance_test.py` - Quick performance check
- `generate_performance_report.py` - Detailed performance analysis

### ðŸ”§ **Investigation Tools** (One-time Use)
**Purpose**: Debug specific issues, explore data  
**When to run**: As needed for troubleshooting  
**Expected runtime**: Variable

- `debug_sync_issue.py` - Debug specific sync problems
- `investigate_conversation_updates.py` - Deep dive into update patterns
- `quick_test.py` - Quick ad-hoc testing
- Various simple_analysis scripts

### ðŸ“ˆ **Baseline Tools** (Comparison)
**Purpose**: Track changes over time, regression detection  
**When to run**: Before major changes, periodic snapshots  
**Expected runtime**: 1-3 minutes

- `record_baseline.py` - Capture current performance baseline
- `compare_to_baseline.py` - Compare against recorded baseline

## Testing Workflow

### **Daily Development**
```bash
# Quick feedback loop
./scripts/unit/pre_commit_validation.sh --fast
./scripts/integration/run_integration_test.sh --days 1
```

### **Before PR Submission**
```bash
# Full validation
./scripts/unit/pre_commit_validation.sh
./scripts/integration/run_integration_test.sh --days 7
./scripts/analysis/content_verification.py
```

### **Investigation/Debugging**
```bash
# Content analysis
./scripts/analysis/analyze_customer_messages.py --date 2025-06-30
./scripts/analysis/content_verification.py --detailed

# Performance investigation  
./scripts/performance/quick_performance_test.py
```

### **Release Preparation**
```bash
# Comprehensive testing
./scripts/integration/run_integration_test_detailed.sh --days 7
./scripts/performance/performance_test.py
./scripts/integration/test_docker_install.sh --with-api-test
```

## Usage Guidelines

1. **Integration tests** should ALWAYS pass before merging
2. **Analysis scripts** help verify data quality but don't block deployments
3. **Investigation tools** are for troubleshooting only
4. **Performance tests** verify we meet targets but allow reasonable variance
5. **Baseline tools** help track trends over time

## Next Steps

- [ ] Reorganize existing scripts into this structure
- [ ] Create `content_verification.py` for comprehensive content analysis
- [ ] Update integration test to include content verification step
- [ ] Add automation to run appropriate tests based on changes